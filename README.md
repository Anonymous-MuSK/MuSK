# MuSK

Given a trained deep Graph Convolution Network (GCN), how can we effectively compress it into a compact network without significant loss of accuracy? Though many deep GCN models improved their performance by considering multi-hop features in graphs, it is difficult to use them in environments having limited computing resources such as mobile or embedded systems. Therefore, it is important to compress large GCN models into a compact GCN model. Knowledge Distillation (KD) is an actively pursued area of research to learn a compressed student model from a teacher model. However, existing KD methods do not preserve the multi-hop aggregation of deep GCN models.
We propose MuSK, a novel approach for compressing deep GCNs through distilling the knowledge of the aggregation from multi-staged GCN layers as well as task prediction. MuSK compresses the deep teacher layers without losing the concept of multi-hop feature aggregation process with a single effective layer in the student. Extensive experiments show that MuSK achieves state-of-the-art performance among other KD based methods while requiring up to 11.4Ã— fewer parameters than the teacher.


# Citation dataset

## Dependencies
- CUDA 10.1
- python 3.6.8
- pytorch 1.7.0
- torch-geometric 1.6.1

## Datasets
The `data` folder contains three benchmark datasets(Cora, Citeseer, Pubmed)
We use the same semi-supervised setting as [GCN](https://github.com/tkipf/gcn)

## Simple Demo
You can run the demo sript by `bash demo.sh`.
It trains MuSK on Cora, Citetation, Pubmed.
This demo saves the trained model at `./student/student_{DATASET}{#LAYERS}.pt`.
Then, it evaluates the trained model in terms of accuracy. 

## Results of MuSK using Pre-trained Teacher
The experimental results with the pre-trained teachers are as follows:

| **Dataset**      |   **Accuracy** |
|:--------------:    |:------:    |
| **Cora**    | 84.70     |
| **Citeseer**   | 72.41     |
| **Pubmed**         | 79.90     |

### Used Hyperparameters 
We briefly summarize the hyperparameters.

* Hyperparameters of MuSK
    - `data`: name of the dataset
    - `layer`: number of layers
    - `test`: evaluation on test dataset
    - `t_hidden`: teacher's hidden feature dimension
    - `s_hidden`: student's hidden feature dimension
    - `lamda`: lamda in GCNII
    - `dropout`: ratio of dropout
    - `wd1`: weight decay
    - `lbd_pred`: lambda for the prediction loss
    - `lbd_embd`: lambda for the embedding loss
    - `kernel`: kernel function

### How to Reproduce the Above Results with the Pre-trained teachers
You can reproduce the results with the following command which evaluates a test dataset using a pre-trained model. 
```shell
python -u student_train.py --data cora --layer 64 --test --lbd_pred 1 --lbd_embd 0.01 --kernel kl
python -u student_train.py --data citeseer --layer 64 --t_hidden 256 --s_hidden 256 --lamda 0.6 --dropout 0.7 --test --lbd_pred 0.1 --lbd_embd 0.01 --kernel kl
python -u student_train.py --data pubmed --layer 64 --t_hidden 256 --s_hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --test --lbd_pred 100 --lbd_embd 10 --kernel kl
```

The pre-trained teachers were generated by the following command:
```shell
python -u teacher_train.py --data cora --layer 64 --test
python -u teacher_train.py --data citeseer --layer 64 --hidden 256 --lamda 0.6 --dropout 0.7 --test
python -u teacher_train.py --data pubmed --layer 64 --hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --test
```

## Reference implementation
Codes are written based on [GCNII](https://github.com/chennnM/GCNII)


# ogbn-proteins dataset

## Dependencies
- CUDA 10.0
- python 3.6.8
- pytorch 1.4.0
- torch-geometric 1.6.0

## Datasets
We use the [ogbn-proteins dataset](https://ogb.stanford.edu/docs/nodeprop/).
When you first run our script, the dataset will be downloaded automatically.

## Usage
To reproduce the results, run the following script.
```sh
python s_train.py
```

## Reference implementation
Codes are written based on [deeperGCN](https://github.com/lightaime/deep_gcns_torch) and pytorch-geometric (https://github.com/rusty1s/pytorch_geometric)
