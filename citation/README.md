# Citation dataset

## Dependencies
- CUDA 10.1
- python 3.6.8
- pytorch 1.7.0
- torch-geometric 1.6.1

## Datasets
The `data` folder contains three benchmark datasets(Cora, Citeseer, Pubmed)
We use the same semi-supervised setting as [GCN](https://github.com/tkipf/gcn)

## Simple Demo
You can run the demo sript by `bash demo.sh`.
It trains MuSK on Cora, Citetation, Pubmed.
This demo saves the trained model at `./student/student_{DATASET}{#LAYERS}.pt`.
Then, it evaluates the trained model in terms of accuracy. 

## Results of MuSK using Pre-trained Teacher
The experimental results with the pre-trained teachers are as follows:

| **Dataset**      |   **Accuracy** |
|:--------------:    |:------:    |
| **Cora**    | 84.70     |
| **Citeseer**   | 72.41     |
| **Pubmed**         | 79.90     |

### Used Hyperparameters 
We briefly summarize the hyperparameters.

* Hyperparameters of MuSK
    - `data`: name of the dataset
    - `layer`: number of layers
    - `test`: evaluation on test dataset
    - `t_hidden`: teacher's hidden feature dimension
    - `s_hidden`: student's hidden feature dimension
    - `lamda`: lamda in GCNII
    - `dropout`: ratio of dropout
    - `wd1`: weight decay
    - `lbd_pred`: lambda for the prediction loss
    - `lbd_embd`: lambda for the embedding loss
    - `kernel`: kernel function

### How to Reproduce the Above Results with the Pre-trained teachers
You can reproduce the results with the following command which evaluates a test dataset using a pre-trained model. 
```shell
python -u student_train.py --data cora --layer 64 --test --lbd_pred 1 --lbd_embd 0.01 --kernel kl
python -u student_train.py --data citeseer --layer 64 --t_hidden 256 --s_hidden 256 --lamda 0.6 --dropout 0.7 --test --lbd_pred 0.1 --lbd_embd 0.01 --kernel kl
python -u student_train.py --data pubmed --layer 64 --t_hidden 256 --s_hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --test --lbd_pred 100 --lbd_embd 10 --kernel kl
```

The pre-trained teachers were generated by the following command:
```shell
#python -u teacher_train.py --data cora --layer 64 --test
#python -u teacher_train.py --data citeseer --layer 64 --hidden 256 --lamda 0.6 --dropout 0.7 --test
#python -u teacher_train.py --data pubmed --layer 64 --hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --test
```

## Reference implementation
Codes are written based on [GCNII](https://github.com/chennnM/GCNII)
